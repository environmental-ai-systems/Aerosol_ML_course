{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Machine Learning for Aerosol Scientists\n",
    "## Part 2: Deep Neural Networks for UVLIF Aerosol Classification\n",
    "\n",
    "### Learning Objectives\n",
    "By the end of this notebook, you will understand:\n",
    "1. How neural networks work (forward propagation, backpropagation)\n",
    "2. Network architecture design (layers, neurons, activation functions)\n",
    "3. Training dynamics (loss curves, overfitting, regularization)\n",
    "4. Comparing DNNs to traditional ML (like XGBoost)\n",
    "\n",
    "### Background\n",
    "While XGBoost is excellent for structured data, deep neural networks (DNNs) offer a different approach:\n",
    "- Learn hierarchical feature representations\n",
    "- Highly flexible architecture\n",
    "- Can capture complex non-linear patterns\n",
    "\n",
    "We'll use the same UVLIF classification task to compare approaches directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install tensorflow numpy pandas matplotlib seaborn scikit-learn -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style and random seeds for reproducibility\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU available: {tf.config.list_physical_devices('GPU')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Generate the Same Synthetic Data\n",
    "\n",
    "We'll use identical data generation to ensure fair comparison with XGBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_uvlif_spectrum(aerosol_type, n_wavelengths=64, noise_level=0.1):\n",
    "    \"\"\"Generate synthetic UVLIF spectrum for different aerosol types.\"\"\"\n",
    "    wavelengths = np.linspace(0, 1, n_wavelengths)\n",
    "    \n",
    "    if aerosol_type == 'biological':\n",
    "        spectrum = (3.0 * np.exp(-((wavelengths - 0.3)**2) / 0.01) + \n",
    "                   4.0 * np.exp(-((wavelengths - 0.5)**2) / 0.02))\n",
    "    elif aerosol_type == 'mineral_dust':\n",
    "        spectrum = 0.5 + 0.3 * np.exp(-((wavelengths - 0.5)**2) / 0.5)\n",
    "    elif aerosol_type == 'organic_carbon':\n",
    "        spectrum = 2.5 * np.exp(-((wavelengths - 0.4)**2) / 0.03)\n",
    "    elif aerosol_type == 'pah':\n",
    "        spectrum = 5.0 * np.exp(-((wavelengths - 0.7)**2) / 0.02)\n",
    "    \n",
    "    noise = noise_level * np.random.randn(n_wavelengths)\n",
    "    spectrum = spectrum + noise\n",
    "    spectrum = np.maximum(spectrum, 0)\n",
    "    \n",
    "    return spectrum\n",
    "\n",
    "# Generate dataset\n",
    "np.random.seed(42)\n",
    "n_samples_per_class = 250\n",
    "aerosol_types = ['biological', 'mineral_dust', 'organic_carbon', 'pah']\n",
    "\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "for aerosol_type in aerosol_types:\n",
    "    for _ in range(n_samples_per_class):\n",
    "        spectrum = generate_uvlif_spectrum(aerosol_type)\n",
    "        data.append(spectrum)\n",
    "        labels.append(aerosol_type)\n",
    "\n",
    "X = np.array(data)\n",
    "y = np.array(labels)\n",
    "\n",
    "print(f\"Dataset shape: {X.shape}\")\n",
    "print(f\"Classes: {aerosol_types}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Prepare Data for Neural Networks\n",
    "\n",
    "Neural networks require:\n",
    "- Scaled inputs (typically normalized to similar ranges)\n",
    "- One-hot encoded labels for multi-class classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test/validation split (same as before)\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.25, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Encode labels to integers\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_val_encoded = label_encoder.transform(y_val)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "# Convert to one-hot encoding for neural network\n",
    "n_classes = len(aerosol_types)\n",
    "y_train_onehot = keras.utils.to_categorical(y_train_encoded, n_classes)\n",
    "y_val_onehot = keras.utils.to_categorical(y_val_encoded, n_classes)\n",
    "y_test_onehot = keras.utils.to_categorical(y_test_encoded, n_classes)\n",
    "\n",
    "print(f\"Training samples: {X_train_scaled.shape[0]}\")\n",
    "print(f\"Validation samples: {X_val_scaled.shape[0]}\")\n",
    "print(f\"Test samples: {X_test_scaled.shape[0]}\")\n",
    "print(f\"\\nInput shape: {X_train_scaled.shape[1]} features\")\n",
    "print(f\"Output shape: {n_classes} classes (one-hot encoded)\")\n",
    "print(f\"\\nExample one-hot encoding: {y_train_onehot[0]} = {label_encoder.classes_[y_train_encoded[0]]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Build a Simple Neural Network\n",
    "\n",
    "### Neural Network Basics:\n",
    "\n",
    "**Architecture components:**\n",
    "1. **Input Layer**: Receives the 64 wavelength channels\n",
    "2. **Hidden Layers**: Process information (learn patterns)\n",
    "3. **Output Layer**: 4 neurons (one per aerosol class)\n",
    "\n",
    "**Key concepts:**\n",
    "- **Neurons**: Basic computational units\n",
    "- **Weights**: Learned parameters connecting neurons\n",
    "- **Activation Functions**: Introduce non-linearity (ReLU, softmax)\n",
    "- **Dropout**: Randomly disable neurons during training (prevents overfitting)\n",
    "\n",
    "Let's start with a simple architecture: Input → Dense(128) → Dense(64) → Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_simple_model(input_dim, n_classes):\n",
    "    \"\"\"\n",
    "    Create a simple feedforward neural network.\n",
    "    \n",
    "    Architecture:\n",
    "    - Input layer: 64 features\n",
    "    - Hidden layer 1: 128 neurons with ReLU activation\n",
    "    - Dropout: 30% (regularization)\n",
    "    - Hidden layer 2: 64 neurons with ReLU activation\n",
    "    - Dropout: 30%\n",
    "    - Output layer: 4 neurons with softmax (probability distribution)\n",
    "    \"\"\"\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=(input_dim,)),\n",
    "        \n",
    "        # First hidden layer\n",
    "        layers.Dense(128, activation='relu', name='hidden_layer_1'),\n",
    "        layers.Dropout(0.3, name='dropout_1'),\n",
    "        \n",
    "        # Second hidden layer\n",
    "        layers.Dense(64, activation='relu', name='hidden_layer_2'),\n",
    "        layers.Dropout(0.3, name='dropout_2'),\n",
    "        \n",
    "        # Output layer\n",
    "        layers.Dense(n_classes, activation='softmax', name='output_layer')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create the model\n",
    "simple_model = create_simple_model(X_train_scaled.shape[1], n_classes)\n",
    "\n",
    "# Display architecture\n",
    "simple_model.summary()\n",
    "\n",
    "print(\"\\nArchitecture explained:\")\n",
    "print(\"- Total parameters: These are learned during training\")\n",
    "print(\"- Dropout layers: Prevent overfitting by randomly disabling connections\")\n",
    "print(\"- Softmax output: Converts to probability distribution (sums to 1.0)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Compile and Train the Model\n",
    "\n",
    "**Training process:**\n",
    "1. **Forward pass**: Input → predictions\n",
    "2. **Loss calculation**: How wrong are predictions?\n",
    "3. **Backpropagation**: Calculate gradients\n",
    "4. **Weight update**: Adjust weights to reduce loss\n",
    "\n",
    "**Key training parameters:**\n",
    "- **Optimizer (Adam)**: Algorithm for updating weights\n",
    "- **Loss function (categorical crossentropy)**: Measures prediction error\n",
    "- **Batch size**: Number of samples processed before updating weights\n",
    "- **Epochs**: Number of complete passes through training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "simple_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Define callbacks\n",
    "early_stopping = callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=15,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "reduce_lr = callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=5,\n",
    "    min_lr=1e-7,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"Training the model...\\n\")\n",
    "print(\"Callbacks enabled:\")\n",
    "print(\"- Early Stopping: Stops if validation loss doesn't improve for 15 epochs\")\n",
    "print(\"- Learning Rate Reduction: Reduces learning rate when stuck\\n\")\n",
    "\n",
    "# Train model\n",
    "history = simple_model.fit(\n",
    "    X_train_scaled, y_train_onehot,\n",
    "    validation_data=(X_val_scaled, y_val_onehot),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stopping, reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Analyze Training History\n",
    "\n",
    "**Learning curves** show how the model learns over time:\n",
    "- **Training loss/accuracy**: Performance on training data\n",
    "- **Validation loss/accuracy**: Performance on unseen validation data\n",
    "\n",
    "**What to look for:**\n",
    "- Training and validation curves should be close (no overfitting)\n",
    "- Both should improve over time\n",
    "- Large gap = overfitting (model memorized training data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(history.history['loss'], label='Training Loss', linewidth=2)\n",
    "axes[0].plot(history.history['val_loss'], label='Validation Loss', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0].set_ylabel('Loss', fontsize=12)\n",
    "axes[0].set_title('Model Loss During Training', fontsize=13, fontweight='bold')\n",
    "axes[0].legend(fontsize=10)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "axes[1].plot(history.history['accuracy'], label='Training Accuracy', linewidth=2)\n",
    "axes[1].plot(history.history['val_accuracy'], label='Validation Accuracy', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1].set_ylabel('Accuracy', fontsize=12)\n",
    "axes[1].set_title('Model Accuracy During Training', fontsize=13, fontweight='bold')\n",
    "axes[1].legend(fontsize=10)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_history_simple.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Print final metrics\n",
    "final_train_acc = history.history['accuracy'][-1]\n",
    "final_val_acc = history.history['val_accuracy'][-1]\n",
    "print(f\"\\nFinal Training Accuracy: {final_train_acc:.4f}\")\n",
    "print(f\"Final Validation Accuracy: {final_val_acc:.4f}\")\n",
    "print(f\"Gap (overfitting indicator): {abs(final_train_acc - final_val_acc):.4f}\")\n",
    "\n",
    "if abs(final_train_acc - final_val_acc) < 0.05:\n",
    "    print(\"✓ Good! Small gap indicates the model generalizes well.\")\n",
    "else:\n",
    "    print(\"⚠ Large gap suggests some overfitting.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Evaluate on Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions\n",
    "y_val_proba = simple_model.predict(X_val_scaled, verbose=0)\n",
    "y_val_pred = np.argmax(y_val_proba, axis=1)\n",
    "\n",
    "# Calculate accuracy\n",
    "val_accuracy = (y_val_pred == y_val_encoded).mean()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"VALIDATION SET PERFORMANCE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nValidation Accuracy: {val_accuracy:.4f} ({val_accuracy*100:.2f}%)\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_val_encoded, y_val_pred,\n",
    "                          target_names=label_encoder.classes_))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_val_encoded, y_val_pred)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=label_encoder.classes_,\n",
    "            yticklabels=label_encoder.classes_)\n",
    "plt.title('Confusion Matrix - Simple DNN', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('True Label', fontsize=12)\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig('confusion_matrix_dnn_simple.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC curves\n",
    "y_val_bin = label_binarize(y_val_encoded, classes=range(n_classes))\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "colors = ['blue', 'red', 'green', 'orange']\n",
    "\n",
    "for i, (color, aerosol_type) in enumerate(zip(colors, aerosol_types)):\n",
    "    fpr, tpr, _ = roc_curve(y_val_bin[:, i], y_val_proba[:, i])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, color=color, lw=2,\n",
    "             label=f'{aerosol_type.replace(\"_\", \" \").title()} (AUC = {roc_auc:.3f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2, label='Random (AUC = 0.5)')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate', fontsize=12)\n",
    "plt.ylabel('True Positive Rate', fontsize=12)\n",
    "plt.title('ROC Curves - Simple DNN', fontsize=14, fontweight='bold')\n",
    "plt.legend(loc=\"lower right\", fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('roc_curves_dnn_simple.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Build a Deeper Network\n",
    "\n",
    "Let's try a more complex architecture to see if we can improve performance:\n",
    "- More layers (deeper network)\n",
    "- Batch normalization (stabilizes training)\n",
    "- Different dropout rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_deep_model(input_dim, n_classes):\n",
    "    \"\"\"\n",
    "    Create a deeper neural network with batch normalization.\n",
    "    \n",
    "    Architecture:\n",
    "    - Deeper: 4 hidden layers instead of 2\n",
    "    - Batch normalization after each layer\n",
    "    - Progressive size reduction (256→128→64→32)\n",
    "    \"\"\"\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=(input_dim,)),\n",
    "        \n",
    "        # Layer 1\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.4),\n",
    "        \n",
    "        # Layer 2\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.3),\n",
    "        \n",
    "        # Layer 3\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.3),\n",
    "        \n",
    "        # Layer 4\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.2),\n",
    "        \n",
    "        # Output\n",
    "        layers.Dense(n_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create deep model\n",
    "deep_model = create_deep_model(X_train_scaled.shape[1], n_classes)\n",
    "deep_model.summary()\n",
    "\n",
    "print(\"\\nKey differences from simple model:\")\n",
    "print(\"- 4 hidden layers vs 2 (more capacity to learn patterns)\")\n",
    "print(\"- Batch normalization (stabilizes training, allows higher learning rates)\")\n",
    "print(\"- More parameters (more flexible but risk of overfitting)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile deep model\n",
    "deep_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Train deep model\n",
    "print(\"Training deeper network...\\n\")\n",
    "\n",
    "history_deep = deep_model.fit(\n",
    "    X_train_scaled, y_train_onehot,\n",
    "    validation_data=(X_val_scaled, y_val_onehot),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stopping, reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Deep model training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare training histories\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Simple model - Loss\n",
    "axes[0, 0].plot(history.history['loss'], label='Training', linewidth=2)\n",
    "axes[0, 0].plot(history.history['val_loss'], label='Validation', linewidth=2)\n",
    "axes[0, 0].set_title('Simple Model - Loss', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Simple model - Accuracy\n",
    "axes[0, 1].plot(history.history['accuracy'], label='Training', linewidth=2)\n",
    "axes[0, 1].plot(history.history['val_accuracy'], label='Validation', linewidth=2)\n",
    "axes[0, 1].set_title('Simple Model - Accuracy', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Accuracy')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Deep model - Loss\n",
    "axes[1, 0].plot(history_deep.history['loss'], label='Training', linewidth=2, color='darkred')\n",
    "axes[1, 0].plot(history_deep.history['val_loss'], label='Validation', linewidth=2, color='red')\n",
    "axes[1, 0].set_title('Deep Model - Loss', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('Loss')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Deep model - Accuracy\n",
    "axes[1, 1].plot(history_deep.history['accuracy'], label='Training', linewidth=2, color='darkred')\n",
    "axes[1, 1].plot(history_deep.history['val_accuracy'], label='Validation', linewidth=2, color='red')\n",
    "axes[1, 1].set_title('Deep Model - Accuracy', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Epoch')\n",
    "axes[1, 1].set_ylabel('Accuracy')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate deep model\n",
    "y_val_proba_deep = deep_model.predict(X_val_scaled, verbose=0)\n",
    "y_val_pred_deep = np.argmax(y_val_proba_deep, axis=1)\n",
    "val_accuracy_deep = (y_val_pred_deep == y_val_encoded).mean()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"DEEP MODEL VALIDATION PERFORMANCE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nValidation Accuracy: {val_accuracy_deep:.4f} ({val_accuracy_deep*100:.2f}%)\")\n",
    "print(\"\\nComparison:\")\n",
    "print(f\"Simple Model: {val_accuracy:.4f}\")\n",
    "print(f\"Deep Model: {val_accuracy_deep:.4f}\")\n",
    "print(f\"Improvement: {(val_accuracy_deep - val_accuracy)*100:.2f} percentage points\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_val_encoded, y_val_pred_deep,\n",
    "                          target_names=label_encoder.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Final Evaluation on Test Set\n",
    "\n",
    "Let's evaluate our best model on the held-out test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the better model (compare validation accuracies)\n",
    "if val_accuracy_deep >= val_accuracy:\n",
    "    best_nn_model = deep_model\n",
    "    model_name = \"Deep Model\"\n",
    "else:\n",
    "    best_nn_model = simple_model\n",
    "    model_name = \"Simple Model\"\n",
    "\n",
    "print(f\"Selected {model_name} for final evaluation\\n\")\n",
    "\n",
    "# Test set evaluation\n",
    "y_test_proba = best_nn_model.predict(X_test_scaled, verbose=0)\n",
    "y_test_pred = np.argmax(y_test_proba, axis=1)\n",
    "test_accuracy = (y_test_pred == y_test_encoded).mean()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"FINAL TEST SET EVALUATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nTest Set Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_encoded, y_test_pred,\n",
    "                          target_names=label_encoder.classes_))\n",
    "\n",
    "# Test set confusion matrix\n",
    "cm_test = confusion_matrix(y_test_encoded, y_test_pred)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm_test, annot=True, fmt='d', cmap='Greens',\n",
    "            xticklabels=label_encoder.classes_,\n",
    "            yticklabels=label_encoder.classes_)\n",
    "plt.title(f'Confusion Matrix - {model_name} (Test Set)', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('True Label', fontsize=12)\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig('confusion_matrix_dnn_test.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test set ROC curves\n",
    "y_test_bin = label_binarize(y_test_encoded, classes=range(n_classes))\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "for i, (color, aerosol_type) in enumerate(zip(colors, aerosol_types)):\n",
    "    fpr, tpr, _ = roc_curve(y_test_bin[:, i], y_test_proba[:, i])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, color=color, lw=2,\n",
    "             label=f'{aerosol_type.replace(\"_\", \" \").title()} (AUC = {roc_auc:.3f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2, label='Random (AUC = 0.5)')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate', fontsize=12)\n",
    "plt.ylabel('True Positive Rate', fontsize=12)\n",
    "plt.title(f'ROC Curves - {model_name} (Test Set)', fontsize=14, fontweight='bold')\n",
    "plt.legend(loc=\"lower right\", fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('roc_curves_dnn_test.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Visualize What the Network Learned\n",
    "\n",
    "Let's examine the first layer weights to understand what patterns the network detects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get first layer weights\n",
    "first_layer_weights = best_nn_model.layers[0].get_weights()[0]  # Shape: (64, n_neurons)\n",
    "\n",
    "# Visualize first 16 neurons' weights\n",
    "fig, axes = plt.subplots(4, 4, figsize=(15, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i in range(16):\n",
    "    axes[i].plot(first_layer_weights[:, i], linewidth=2)\n",
    "    axes[i].set_title(f'Neuron {i+1}', fontsize=10)\n",
    "    axes[i].set_xlabel('Wavelength Channel', fontsize=8)\n",
    "    axes[i].set_ylabel('Weight', fontsize=8)\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "    axes[i].tick_params(labelsize=7)\n",
    "\n",
    "plt.suptitle('First Layer Weight Patterns - What Each Neuron Detects', \n",
    "             fontsize=14, fontweight='bold', y=1.00)\n",
    "plt.tight_layout()\n",
    "plt.savefig('first_layer_weights.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"Each neuron learns to detect specific spectral patterns.\")\n",
    "print(\"Some neurons respond to peaks, others to valleys or specific wavelength ranges.\")\n",
    "print(\"These learned features are then combined in deeper layers for classification.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Compare with XGBoost Results\n",
    "\n",
    "Let's create a comprehensive comparison between approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison summary\n",
    "comparison_data = {\n",
    "    'Metric': ['Test Accuracy', 'Mean AUC', 'Training Time', 'Interpretability', \n",
    "               'Hyperparameter Tuning', 'Data Efficiency'],\n",
    "    'XGBoost': ['~97-99%', '~0.99', 'Fast (minutes)', 'High (feature importance)', \n",
    "                'Critical', 'Good with small data'],\n",
    "    'Deep Neural Network': [f'{test_accuracy*100:.1f}%', \n",
    "                           f'{np.mean([auc(fpr, tpr) for fpr, tpr in [(roc_curve(y_test_bin[:, i], y_test_proba[:, i])[:2]) for i in range(n_classes)]]):.2f}',\n",
    "                           'Moderate (minutes)', 'Lower (black box)', \n",
    "                           'Important', 'Needs more data for best results']\n",
    "}\n",
    "\n",
    "df_comparison = pd.DataFrame(comparison_data)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPARISON: XGBoost vs Deep Neural Networks\")\n",
    "print(\"=\"*80)\n",
    "print(df_comparison.to_string(index=False))\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Key Takeaways\n",
    "\n",
    "### What We Learned About Neural Networks:\n",
    "\n",
    "1. **Architecture Design**\n",
    "   - Layers and neurons control model capacity\n",
    "   - Deeper ≠ always better (risk of overfitting)\n",
    "   - Batch normalization and dropout are crucial regularization techniques\n",
    "\n",
    "2. **Training Dynamics**\n",
    "   - Learning curves reveal overfitting vs underfitting\n",
    "   - Early stopping prevents overtraining\n",
    "   - Learning rate scheduling helps convergence\n",
    "\n",
    "3. **When to Use DNNs vs XGBoost**\n",
    "\n",
    "   **Use XGBoost when:**\n",
    "   - Working with tabular/structured data\n",
    "   - Need interpretability (feature importance)\n",
    "   - Have limited data\n",
    "   - Want fast training and tuning\n",
    "   - Need robust baseline quickly\n",
    "   \n",
    "   **Use Deep Neural Networks when:**\n",
    "   - Working with images, sequences, or unstructured data\n",
    "   - Have large datasets\n",
    "   - Need to learn complex hierarchical features\n",
    "   - Can invest time in architecture search\n",
    "   - Transfer learning is an option\n",
    "\n",
    "### For Aerosol Mass Spectra:\n",
    "- Both methods work excellently (~97-99% accuracy)\n",
    "- XGBoost might be preferred due to:\n",
    "  - Simpler to tune\n",
    "  - Faster training\n",
    "  - Better interpretability\n",
    "  - Works well with our data size\n",
    "- DNNs show their strength with larger, more complex datasets\n",
    "\n",
    "### Next Steps in Your ML Journey:\n",
    "1. Try these methods on your real UVLIF data\n",
    "2. Experiment with different architectures\n",
    "3. Explore ensemble methods (combining models)\n",
    "4. Look into transfer learning for related aerosol tasks\n",
    "5. Consider 1D Convolutional Neural Networks for spectral data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best neural network model\n",
    "best_nn_model.save('best_dnn_model.keras')\n",
    "print(\"✓ Model saved to 'best_dnn_model.keras'\")\n",
    "print(\"\\nYou can load it later with: model = keras.models.load_model('best_dnn_model.keras')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Example Prediction Function\n",
    "\n",
    "Here's how you would use the trained model for new predictions in practice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_aerosol_type(spectrum, model, scaler, label_encoder):\n",
    "    \"\"\"\n",
    "    Predict aerosol type from a UVLIF spectrum.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    spectrum : array-like, shape (n_wavelengths,)\n",
    "        UVLIF fluorescence spectrum\n",
    "    model : keras.Model\n",
    "        Trained neural network\n",
    "    scaler : StandardScaler\n",
    "        Fitted scaler for normalization\n",
    "    label_encoder : LabelEncoder\n",
    "        Label encoder for class names\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    prediction : str\n",
    "        Predicted aerosol type\n",
    "    probabilities : dict\n",
    "        Probability for each class\n",
    "    \"\"\"\n",
    "    # Ensure 2D shape\n",
    "    spectrum_2d = np.array(spectrum).reshape(1, -1)\n",
    "    \n",
    "    # Scale\n",
    "    spectrum_scaled = scaler.transform(spectrum_2d)\n",
    "    \n",
    "    # Predict\n",
    "    proba = model.predict(spectrum_scaled, verbose=0)[0]\n",
    "    pred_class_idx = np.argmax(proba)\n",
    "    pred_class = label_encoder.classes_[pred_class_idx]\n",
    "    \n",
    "    # Create probability dictionary\n",
    "    prob_dict = {label_encoder.classes_[i]: float(proba[i]) \n",
    "                 for i in range(len(label_encoder.classes_))}\n",
    "    \n",
    "    return pred_class, prob_dict\n",
    "\n",
    "# Test with a random sample\n",
    "test_idx = np.random.randint(0, len(X_test))\n",
    "test_spectrum = X_test[test_idx]\n",
    "true_label = y_test[test_idx]\n",
    "\n",
    "predicted_label, probabilities = predict_aerosol_type(\n",
    "    test_spectrum, best_nn_model, scaler, label_encoder\n",
    ")\n",
    "\n",
    "print(\"\\nExample Prediction:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"True Label: {true_label}\")\n",
    "print(f\"Predicted Label: {predicted_label}\")\n",
    "print(f\"\\nClass Probabilities:\")\n",
    "for aerosol_type, prob in sorted(probabilities.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"  {aerosol_type:20s}: {prob*100:5.2f}%\")\n",
    "\n",
    "# Visualize the spectrum\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(test_spectrum, linewidth=2, color='steelblue')\n",
    "plt.xlabel('Wavelength Channel', fontsize=12)\n",
    "plt.ylabel('Fluorescence Intensity', fontsize=12)\n",
    "plt.title(f'Example Spectrum: True={true_label}, Predicted={predicted_label}', \n",
    "          fontsize=13, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
